{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM3d/NBJl+yS8SvVLrCiJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/MachineLearning-KISD-2024/blob/main/lecture6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSZKkOoLKIb0"
      },
      "source": [
        "<center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**<font color = \"red\">Introduction to machine learning</font>**\n",
        "\n",
        "**Lecture 6**\n",
        "\n",
        "\n",
        "##**<font color = \"green\">Reinforcement learning</font>**\n",
        "\n",
        "*17 April 2024*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Marcin Wolter, IFJ PAN*\n",
        "\n",
        "*e-mail: marcin.wolter@ifj.edu.pl*\n",
        "\n",
        "\n",
        "---\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zv8I7LqIPOD"
      },
      "source": [
        "#<font color='green'>**Program for today:**\n",
        "\n",
        "\n",
        "* ###  <font color='red'>Reinforcement learning: how to train a robot?\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**As always all slides are here:**\n",
        "\n",
        "*https://github.com/marcinwolter/MachineLearning-KISD-2024*\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='green'> **Reinforcement learning**"
      ],
      "metadata": {
        "id": "UBgPgOqJB_em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Definition**\n",
        "Reinforcement Learning (RL) is the science of decision making. It is about learning the optimal behavior in an environment to obtain maximum reward. This optimal behavior is learned through interactions with the environment and observations of how it responds, similar to children exploring the world around them and learning the actions that help them achieve a goal.\n",
        "\n",
        "In the absence of a supervisor, the learner must independently discover the sequence of actions that maximize the reward. This discovery process is akin to a trial-and-error search. The quality of actions is measured by not just the immediate reward they return, but also the delayed reward they might fetch. As it can learn the actions that result in eventual success in an unseen environment without the help of a supervisor, reinforcement learning is a very powerful algorithm."
      ],
      "metadata": {
        "id": "qrRtsLoxCIZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How Does Reinforcement Learning Work?**\n",
        "The Reinforcement Learning problem involves an agent exploring an unknown environment to achieve a goal. RL is based on the hypothesis that all goals can be described by the maximization of expected cumulative reward. The agent must learn to sense and perturb the state of the environment using its actions to derive maximal reward. The formal framework for RL borrows from the problem of optimal control of Markov Decision Processes (MDP).\n",
        "\n",
        "The main elements of an RL system are:\n",
        "\n",
        "1. The agent or the learner\n",
        "2. The environment the agent interacts with\n",
        "3. The policy that the agent follows to take actions\n",
        "4. The reward signal that the agent observes upon taking actions\n",
        "\n",
        "A useful abstraction of the reward signal is the value function, which faithfully captures the ‘goodness’ of a state. While the reward signal represents the immediate benefit of being in a certain state, the value function captures the cumulative reward that is expected to be collected from that state on, going into the future. The objective of an RL algorithm is to discover the action policy that maximizes the average value that it can extract from every state of the system.\n",
        "\n",
        "<img src='https://images.synopsys.com/is/image/synopsys/reinforcement-learningV1-02?qlt=82&wid=1200&ts=1680107783898&$responsive$&fit=constrain&dpr=off' width=500px>"
      ],
      "metadata": {
        "id": "zzEMkrXjCoKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Examples of Reinforcement Learning**\n",
        "Any real-world problem where an agent must interact with an uncertain environment to meet a specific goal is a potential application of RL. Here are a few RL success stories:\n",
        "\n",
        "1. **Robotics.** Robots with pre-programmed behavior are useful in structured environments, such as the assembly line of an automobile manufacturing plant, where the task is repetitive in nature. In the real world, where the response of the environment to the behavior of the robot is uncertain, pre-programming accurate actions is nearly impossible. In such scenarios, RL provides an efficient way to build general-purpose robots. It has been successfully applied to robotic path planning, where a robot must find a short, smooth, and navigable path between two locations, void of collisions and compatible with the dynamics of the robot.\n",
        "\n",
        "2. **AlphaGo.** One of the most complex strategic games is a 3,000-year-old Chinese board game called Go. Its complexity stems from the fact that there are 10^270 possible board combinations, several orders of magnitude more than the game of chess. In 2016, an RL-based Go agent called AlphaGo defeated the greatest human Go player. Much like a human player, it learned by experience, playing thousands of games with professional players. The latest RL-based Go agent has the capability to learn by playing against itself, an advantage that the human player doesn’t have.\n",
        "\n",
        "3. **Autonomous Driving.** An autonomous driving system must perform multiple perception and planning tasks in an uncertain environment. Some specific tasks where RL finds application include vehicle path planning and motion prediction. Vehicle path planning requires several low and high-level policies to make decisions over varying temporal and spatial scales. Motion prediction is the task of predicting the movement of pedestrians and other vehicles, to understand how the situation might develop based on the current state of the environment."
      ],
      "metadata": {
        "id": "rV6DZsi_Dhkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><strong>How Actor-Critic works:</strong></h2>\n",
        "<p>Imagine you play a video game with a friend that provides you some feedback. You're the Actor, and your friend is the Critic:</p>\n",
        "<p><img class=\"img-fluid\" style=\"display: block; margin-left: auto; margin-right: auto;\" src=\"https://pylessons.com/media/Tutorials/Reinforcement-learning-tutorial/A2C-reinforcement-learning/09_A2C-reinforcement-learning.png\" alt=\"\" width=\"949\" height=\"534\" />\n",
        "\n",
        "In the beginning, you don't know how to play, so you try some action randomly. The Critic observes your action and provides feedback.\n",
        "<p>In the Actor-Critic Methods:</p>\n",
        "<ul>\n",
        "<li>The \"Critic\" estimates the value function. This could be the action-value (the Q value) or state-value (the V value).</li>\n",
        "<li>Critic: Q-learning algorithm that critiques the action that the Actor selected, providing feedback on how to adjust. It can take advantage of efficiency tricks in Q-learning, such as memory replay.</li>\n",
        "</ul>\n",
        "<p>We update both the Critic network and the Value network at each update step.</p>\n",
        "<p>Intuitively, this means how better it is to take a specific action than the average general action at the given state. So, using the Value function as the baseline function, we subtract the Q value term with the Value."
      ],
      "metadata": {
        "id": "FyID3ouGEubV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozshgd3tAlaE"
      },
      "source": [
        "# Actor Critic Method\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "Modified by Marcin Wolter<br>\n",
        "**Date created:** 2020/05/13<br>\n",
        "**Last modified:** 2023/04/16<br>\n",
        "**Description:** Implement Actor Critic Method in CartPole environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n4O8LgMAlaG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n",
        "\n",
        "### Actor Critic Method\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to two possible outputs:\n",
        "\n",
        "1. Recommended action: A probability value for each action in the action space.\n",
        "   The part of the agent responsible for this output is called the **actor**.\n",
        "2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n",
        "   future. The part of the agent responsible for this output is the **critic**.\n",
        "\n",
        "Agent and Critic learn to perform their tasks, such that the recommended actions\n",
        "from the actor maximize the rewards.\n",
        "\n",
        "### CartPole-V1\n",
        "\n",
        "A pole is attached to a cart placed on a frictionless track. The agent has to apply\n",
        "force to move the cart. It is rewarded for every time step the pole\n",
        "remains upright. The agent, therefore, must learn to keep the pole from falling over.\n",
        "\n",
        "### References\n",
        "\n",
        "- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "- [CartPole in GYM](https://www.gymlibrary.dev/environments/classic_control/cart_pole/ )\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cart Pole**\n",
        "<figure class=\"align-default\" id=\"id1\">\n",
        "<a class=\"reference internal image-reference\" href=\"https://www.gymlibrary.dev/_images/cart_pole.gif\"><img alt=\"https://www.gymlibrary.dev/_images/cart_pole.gif\" src=\"https://www.gymlibrary.dev/_images/cart_pole.gif\" style=\"width: 200px;\" /></a>\n",
        "</figure>\n",
        "<p>This environment is part of the <a href='..'>Classic Control environments</a>. Please read that page first for general information.</p>\n",
        "<div class=\"table-wrapper colwidths-auto docutils container\">\n",
        "<table class=\"docutils align-default\">\n",
        "<thead>\n",
        "<tr class=\"row-odd\"><th class=\"head\"><p></p></th>\n",
        "<th class=\"head\"><p></p></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr class=\"row-even\"><td><p>Action Space</p></td>\n",
        "<td><p>Discrete(2)</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p>Observation Shape</p></td>\n",
        "<td><p>(4,)</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p>Observation High</p></td>\n",
        "<td><p>[4.8   inf 0.42  inf]</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p>Observation Low</p></td>\n",
        "<td><p>[-4.8   -inf -0.42  -inf]</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p>Import</p></td>\n",
        "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">gym.make(&quot;CartPole-v1&quot;)</span></code></p></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "<section id=\"description\">\n",
        "<h2>Description</h2>\n",
        "<p>This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
        "<a class=\"reference external\" href=\"https://ieeexplore.ieee.org/document/6313077\">“Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”</a>.\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
        "in the left and right direction on the cart.</p>\n",
        "</section>\n",
        "<section id=\"action-space\">\n",
        "<h2>Action Space</h2>\n",
        "<p>The action is a <code class=\"docutils literal notranslate\"><span class=\"pre\">ndarray</span></code> with shape <code class=\"docutils literal notranslate\"><span class=\"pre\">(1,)</span></code> which can take values <code class=\"docutils literal notranslate\"><span class=\"pre\">{0,</span> <span class=\"pre\">1}</span></code> indicating the direction\n",
        "of the fixed force the cart is pushed with.</p>\n",
        "<div class=\"table-wrapper colwidths-auto docutils container\">\n",
        "<table class=\"docutils align-default\">\n",
        "<thead>\n",
        "<tr class=\"row-odd\"><th class=\"head\"><p>Num</p></th>\n",
        "<th class=\"head\"><p>Action</p></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr class=\"row-even\"><td><p>0</p></td>\n",
        "<td><p>Push cart to the left</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p>1</p></td>\n",
        "<td><p>Push cart to the right</p></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "<p><strong>Note</strong>: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
        "the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it</p>\n",
        "</section>\n",
        "<section id=\"observation-space\">\n",
        "<h2>Observation Space</h2>\n",
        "<p>The observation is a <code class=\"docutils literal notranslate\"><span class=\"pre\">ndarray</span></code> with shape <code class=\"docutils literal notranslate\"><span class=\"pre\">(4,)</span></code> with the values corresponding to the following positions and velocities:</p>\n",
        "<div class=\"table-wrapper colwidths-auto docutils container\">\n",
        "<table class=\"docutils align-default\">\n",
        "<thead>\n",
        "<tr class=\"row-odd\"><th class=\"head\"><p>Num</p></th>\n",
        "<th class=\"head\"><p>Observation</p></th>\n",
        "<th class=\"head\"><p>Min</p></th>\n",
        "<th class=\"head\"><p>Max</p></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr class=\"row-even\"><td><p>0</p></td>\n",
        "<td><p>Cart Position</p></td>\n",
        "<td><p>-4.8</p></td>\n",
        "<td><p>4.8</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p>1</p></td>\n",
        "<td><p>Cart Velocity</p></td>\n",
        "<td><p>-Inf</p></td>\n",
        "<td><p>Inf</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p>2</p></td>\n",
        "<td><p>Pole Angle</p></td>\n",
        "<td><p>~ -0.418 rad (-24°)</p></td>\n",
        "<td><p>~ 0.418 rad (24°)</p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p>3</p></td>\n",
        "<td><p>Pole Angular Velocity</p></td>\n",
        "<td><p>-Inf</p></td>\n",
        "<td><p>Inf</p></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "<p><strong>Note:</strong> While the ranges above denote the possible values for observation space of each element,\n",
        "it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:</p>\n",
        "<ul class=\"simple\">\n",
        "<li><p>The cart x-position (index 0) can be take values between <code class=\"docutils literal notranslate\"><span class=\"pre\">(-4.8,</span> <span class=\"pre\">4.8)</span></code>, but the episode terminates\n",
        "if the cart leaves the <code class=\"docutils literal notranslate\"><span class=\"pre\">(-2.4,</span> <span class=\"pre\">2.4)</span></code> range.</p></li>\n",
        "<li><p>The pole angle can be observed between  <code class=\"docutils literal notranslate\"><span class=\"pre\">(-.418,</span> <span class=\"pre\">.418)</span></code> radians (or <strong>±24°</strong>), but the episode terminates\n",
        "if the pole angle is not in the range <code class=\"docutils literal notranslate\"><span class=\"pre\">(-.2095,</span> <span class=\"pre\">.2095)</span></code> (or <strong>±12°</strong>)</p></li>\n",
        "</ul>\n",
        "</section>\n",
        "<section id=\"rewards\">\n",
        "<h2>Rewards</h2>\n",
        "<p>Since the goal is to keep the pole upright for as long as possible, a reward of <code class=\"docutils literal notranslate\"><span class=\"pre\">+1</span></code> for every step taken,\n",
        "including the termination step, is allotted. The threshold for rewards is 475 for v1.</p>\n",
        "</section>\n",
        "<section id=\"starting-state\">\n",
        "<h2>Starting State</h2>\n",
        "<p>All observations are assigned a uniformly random value in <code class=\"docutils literal notranslate\"><span class=\"pre\">(-0.05,</span> <span class=\"pre\">0.05)</span></code></p>\n",
        "</section>\n",
        "<section id=\"episode-end\">\n",
        "<h2>Episode End</h2>\n",
        "<p>The episode ends if any one of the following occurs:</p>\n",
        "<ol class=\"arabic simple\">\n",
        "<li><p>Termination: Pole Angle is greater than ±12°</p></li>\n",
        "<li><p>Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)</p></li>\n",
        "<li><p>Truncation: Episode length is greater than 500 (200 for v0)</p></li>\n",
        "</ol>\n",
        "</section>\n",
        "<section id=\"arguments\">\n",
        "<h2>Arguments</h2>\n",
        "<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">&#39;CartPole-v1&#39;</span><span class=\"p\">)</span>\n",
        "</pre></div>\n",
        "</div>\n",
        "<p>No additional arguments are currently supported.</p>\n",
        "</section>\n",
        "</section>"
      ],
      "metadata": {
        "id": "v7rNvcyqercp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tXRhXNAlaH"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYzp4i_1AlaI"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from gym import wrappers\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v1\")  # Create the environment  , render_mode='rgb_array'\n",
        "env.reset(seed=seed)\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n"
      ],
      "metadata": {
        "id": "X4NOlXFV6Yel",
        "outputId": "40812191-c497-4124-d2cf-00a303ce8434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYS4spFrAlaJ"
      },
      "source": [
        "## Implement Actor Critic network\n",
        "\n",
        "This network learns two functions:\n",
        "\n",
        "1. Actor: This takes as input the state of our environment and returns a\n",
        "probability value for each action in its action space.\n",
        "2. Critic: This takes as input the state of our environment and returns\n",
        "an estimate of total rewards in the future.\n",
        "\n",
        "In our implementation, they share the initial layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBXhXpq6AlaJ",
        "outputId": "791b6d08-80c1-4381-ee12-c707baec9703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          640         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            258         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            129         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,027\n",
            "Trainable params: 1,027\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_inputs = 4    # four parameters describing the cart state\n",
        "num_actions = 2   # move the cart left or right\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_video(i, env, model):\n",
        "\n",
        "    video = VideoRecorder(env, 'final_'+str(i)+'.mp4')\n",
        "\n",
        "    steps = 0\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        env.render(mode='rgb_array')\n",
        "        video.capture_frame()\n",
        "        # Predict action probabilities and estimated future rewards\n",
        "        # from environment state\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        action_probs, critic_value = model(state)\n",
        "        # Sample action from action probability distribution\n",
        "        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Testing steps: {}: \".format(steps))\n",
        "    video.close()\n"
      ],
      "metadata": {
        "id": "A77k0sXwrWl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz56aqURAlaJ"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuDyHmdaAlaK",
        "outputId": "87ce3def-7b54-46ea-d568-72bb2bc88740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fef41f8e1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fef41f8e1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 10.25 at episode 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 18: \n",
            "running reward: 17.16 at episode 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 21: \n",
            "running reward: 22.37 at episode 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 49: \n",
            "running reward: 26.78 at episode 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 105: \n",
            "running reward: 38.56 at episode 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 121: \n",
            "running reward: 53.16 at episode 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 78: \n",
            "running reward: 44.92 at episode 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 24: \n",
            "running reward: 77.10 at episode 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 32: \n",
            "running reward: 103.78 at episode 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 120: \n",
            "running reward: 133.37 at episode 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 163: \n",
            "Solved at episode 105!\n"
          ]
        }
      ],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "video = VideoRecorder(env, 'training.mp4')\n",
        "while True:  # Run until solved\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); Adding this line would show the attempts\n",
        "            # of the agent in a pop up window.\n",
        "            env.render(mode='rgb_array')\n",
        "            video.capture_frame()  # capture the video frame\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state\n",
        "            action_probs, critic_value = model(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "        make_video(episode_count, env, model)\n",
        "\n",
        "\n",
        "    if running_reward > 195: # 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        video.close()\n",
        "        break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Record video after training**"
      ],
      "metadata": {
        "id": "0JWSvUNigeZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_video(episode_count, env, model)"
      ],
      "metadata": {
        "id": "J_XTQ5OEHOsf",
        "outputId": "cfd12233-9315-497f-c546-1be9b5393326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 500: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **So, we have trained the robot!!!**"
      ],
      "metadata": {
        "id": "Yit-jcW7F_K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Film showing the training of a robot:**\n",
        "\n",
        "[![Blinking LEDs](http://img.youtube.com/vi/n2gE7n11h1Y/0.jpg)](https://www.youtube.com/watch?v=n2gE7n11h1Y \"Blinking LEDs\")"
      ],
      "metadata": {
        "id": "PPW3BY0cMi3j"
      }
    }
  ]
}